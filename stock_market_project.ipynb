{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m3e5dkhi6i",
   "metadata": {},
   "source": [
    "# Target Variables Analysis\n",
    "\n",
    "## 📊 Understanding Our Target Indicators\n",
    "\n",
    "Our model aims to predict three key financial indicators that capture different aspects of market performance:\n",
    "\n",
    "### 1. **Forward Returns** (`forward_returns`)\n",
    "- **Definition**: The next-day return of the S&P 500 index\n",
    "- **Formula**: (Price_tomorrow - Price_today) / Price_today\n",
    "- **Importance**: This is our primary prediction target - the actual market movement we want to forecast\n",
    "- **Range**: Typically between -4% to +4% daily\n",
    "- **Investment Use**: Direct signal for market direction and magnitude\n",
    "\n",
    "### 2. **Risk-Free Rate** (`risk_free_rate`)\n",
    "- **Definition**: The return on risk-free government securities (typically Treasury bills)\n",
    "- **Importance**: Represents the baseline return investors can earn without taking any risk\n",
    "- **Investment Use**: Used to calculate risk premiums and evaluate if market returns justify the risk taken\n",
    "- **Typical Range**: 0% to 0.03% daily (0% to 8% annually)\n",
    "\n",
    "### 3. **Market Forward Excess Returns** (`market_forward_excess_returns`)\n",
    "- **Definition**: Forward returns minus the risk-free rate\n",
    "- **Formula**: forward_returns - risk_free_rate\n",
    "- **Importance**: Measures the **risk premium** - extra return for taking market risk\n",
    "- **Investment Use**: Key metric for risk-adjusted performance evaluation\n",
    "- **Interpretation**: Positive values indicate the market is compensating investors for risk\n",
    "\n",
    "## 🎯 Why These Targets Matter\n",
    "\n",
    "These three indicators together provide a complete picture of market performance:\n",
    "- **Absolute returns** (forward_returns): Raw market movement\n",
    "- **Risk-free benchmark** (risk_free_rate): Opportunity cost of capital\n",
    "- **Risk-adjusted returns** (excess_returns): True economic profit after accounting for risk\n",
    "\n",
    "This framework allows investors to make informed decisions about whether market risk is being adequately compensated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fec56-e4ce-446c-863f-8a5a58ad3b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1. IMPORTS AND SETUP (Essential libraries only)\n",
    "# ===============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set basic matplotlib style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Load data with full file path\n",
    "train_df = pd.read_csv(r\"C:\\Users\\Bhekiz\\Documents\\School\\Independent study\\hull-tactical-market-prediction\\train.csv\")\n",
    "\n",
    "print(\"📊 Dataset loaded successfully!\")\n",
    "print(f\"Training data: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns[:10])}...\")  # Show first 10 column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c5cb3-5304-4618-a0e4-bb23eac19685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BASIC DATASET OVERVIEW TABLE\n",
    "# ===============================\n",
    "def create_dataset_overview():\n",
    "    \"\"\"Create comprehensive dataset overview table\"\"\"\n",
    "    \n",
    "    overview_data = {\n",
    "        'Metric': [\n",
    "            'Total Rows', 'Total Columns', 'Date Range (ID)', \n",
    "            'Memory Usage (MB)', 'Target Variables', 'Feature Variables'\n",
    "        ],\n",
    "        'Training Data': [\n",
    "            f\"{train_df.shape[0]:,}\",\n",
    "            train_df.shape[1],\n",
    "            f\"{train_df['date_id'].min()} - {train_df['date_id'].max()}\",\n",
    "            f\"{train_df.memory_usage(deep=True).sum() / 1024**2:.1f}\",\n",
    "            \"3 (forward_returns, risk_free_rate, market_forward_excess_returns)\",\n",
    "            f\"{train_df.shape[1] - 4} features\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    overview_df = pd.DataFrame(overview_data)\n",
    "    return overview_df\n",
    "\n",
    "overview_table = create_dataset_overview()\n",
    "print(\"\\n📋 DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(overview_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af203086-84a2-4d1c-8568-d40d9f7f7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FEATURE CATEGORY ANALYSIS TABLE\n",
    "# ==================================\n",
    "def analyze_feature_categories():\n",
    "    \"\"\"Analyze features by category with counts and missing data\"\"\"\n",
    "    \n",
    "    # Define feature categories\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']]\n",
    "    \n",
    "    categories = {\n",
    "        'Market Dynamics (M)': [col for col in feature_cols if col.startswith('M')],\n",
    "        'Macro Economic (E)': [col for col in feature_cols if col.startswith('E')],\n",
    "        'Interest Rates (I)': [col for col in feature_cols if col.startswith('I')],\n",
    "        'Price/Valuation (P)': [col for col in feature_cols if col.startswith('P')],\n",
    "        'Volatility (V)': [col for col in feature_cols if col.startswith('V')],\n",
    "        'Sentiment (S)': [col for col in feature_cols if col.startswith('S')],\n",
    "        'Binary/Dummy (D)': [col for col in feature_cols if col.startswith('D')]\n",
    "    }\n",
    "    \n",
    "    category_analysis = []\n",
    "    \n",
    "    for category, features in categories.items():\n",
    "        if features:\n",
    "            missing_pct = (train_df[features].isnull().sum().sum() / (len(features) * len(train_df))) * 100\n",
    "            category_analysis.append({\n",
    "                'Category': category,\n",
    "                'Feature Count': len(features),\n",
    "                'Missing Data (%)': f\"{missing_pct:.1f}%\",\n",
    "                'Example Features': ', '.join(features[:3]) + ('...' if len(features) > 3 else ''),\n",
    "                'Data Type': 'Binary' if category.endswith('(D)') else 'Continuous'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(category_analysis)\n",
    "\n",
    "feature_category_table = analyze_feature_categories()\n",
    "print(\"\\n🎯 FEATURE CATEGORIES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(feature_category_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e757133-e011-4047-a9c1-a7a4d61072e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TARGET VARIABLE STATISTICS TABLE\n",
    "# ===================================\n",
    "def target_variable_stats():\n",
    "    \"\"\"Comprehensive target variable statistics\"\"\"\n",
    "    \n",
    "    target_cols = ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n",
    "    stats_data = []\n",
    "    \n",
    "    for col in target_cols:\n",
    "        data = train_df[col].dropna()\n",
    "        \n",
    "        # Calculate skewness and kurtosis manually (without scipy)\n",
    "        mean = data.mean()\n",
    "        std = data.std()\n",
    "        n = len(data)\n",
    "        \n",
    "        # Skewness calculation\n",
    "        skewness = ((data - mean) ** 3).mean() / (std ** 3)\n",
    "        \n",
    "        # Kurtosis calculation (excess kurtosis)\n",
    "        kurtosis = ((data - mean) ** 4).mean() / (std ** 4) - 3\n",
    "        \n",
    "        stats_data.append({\n",
    "            'Variable': col.replace('_', ' ').title(),\n",
    "            'Count': f\"{len(data):,}\",\n",
    "            'Mean': f\"{data.mean():.6f}\",\n",
    "            'Std Dev': f\"{data.std():.6f}\",\n",
    "            'Min': f\"{data.min():.6f}\",\n",
    "            'Max': f\"{data.max():.6f}\",\n",
    "            'Skewness': f\"{skewness:.3f}\",\n",
    "            'Kurtosis': f\"{kurtosis:.3f}\",\n",
    "            'Annualized Mean (%)': f\"{data.mean() * 252 * 100:.2f}%\" if 'return' in col else 'N/A',\n",
    "            'Annualized Vol (%)': f\"{data.std() * np.sqrt(252) * 100:.2f}%\" if 'return' in col else 'N/A'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats_data)\n",
    "\n",
    "target_stats_table = target_variable_stats()\n",
    "print(\"\\n📈 TARGET VARIABLE STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(target_stats_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c31811-4e43-43fe-ba81-41e99c13976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MISSING DATA ANALYSIS BY TIME PERIOD\n",
    "# =======================================\n",
    "def missing_data_by_period():\n",
    "    \"\"\"Analyze missing data patterns across different time periods\"\"\"\n",
    "    \n",
    "    # Divide data into periods\n",
    "    total_rows = len(train_df)\n",
    "    period_size = total_rows // 4  # 4 periods\n",
    "    \n",
    "    feature_cols = [col for col in train_df.columns if col not in ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']]\n",
    "    \n",
    "    categories = {\n",
    "        'Market (M)': [col for col in feature_cols if col.startswith('M')],\n",
    "        'Economic (E)': [col for col in feature_cols if col.startswith('E')],\n",
    "        'Interest (I)': [col for col in feature_cols if col.startswith('I')],\n",
    "        'Price (P)': [col for col in feature_cols if col.startswith('P')],\n",
    "        'Volatility (V)': [col for col in feature_cols if col.startswith('V')],\n",
    "        'Sentiment (S)': [col for col in feature_cols if col.startswith('S')],\n",
    "        'Binary (D)': [col for col in feature_cols if col.startswith('D')]\n",
    "    }\n",
    "    \n",
    "    periods = {\n",
    "        'Early (0-25%)': slice(0, period_size),\n",
    "        'Early-Mid (25-50%)': slice(period_size, 2*period_size),\n",
    "        'Mid-Late (50-75%)': slice(2*period_size, 3*period_size),\n",
    "        'Recent (75-100%)': slice(3*period_size, total_rows)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n🕐 MISSING DATA BY TIME PERIOD\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create header\n",
    "    header = f\"{'Feature Category':<15}\"\n",
    "    for period in periods.keys():\n",
    "        header += f\"{period:>15}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    # Calculate and display missing percentages\n",
    "    for category, features in categories.items():\n",
    "        if features:\n",
    "            row = f\"{category:<15}\"\n",
    "            for period_name, period_slice in periods.items():\n",
    "                period_data = train_df.iloc[period_slice]\n",
    "                missing_pct = (period_data[features].isnull().sum().sum() / (len(features) * len(period_data))) * 100\n",
    "                row += f\"{missing_pct:>13.1f}%\"\n",
    "            print(row)\n",
    "\n",
    "missing_data_by_period()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c515d86-34c4-4f52-b760-eab2b0a55a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CHART 1: RETURNS DISTRIBUTION AND STATISTICS\n",
    "# ===============================================\n",
    "def plot_returns_analysis():\n",
    "    \"\"\"Comprehensive returns distribution analysis\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('📈 Forward Returns Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    returns = train_df['forward_returns'].dropna()\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0,0].hist(returns, bins=100, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].axvline(returns.mean(), color='red', linestyle='--', label=f'Mean: {returns.mean():.4f}')\n",
    "    axes[0,0].set_title('Returns Distribution')\n",
    "    axes[0,0].set_xlabel('Daily Returns')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q Plot (manual implementation without scipy)\n",
    "    sorted_returns = np.sort(returns)\n",
    "    n = len(sorted_returns)\n",
    "    theoretical_quantiles = np.linspace(0.01, 0.99, n)\n",
    "    normal_quantiles = np.percentile(np.random.normal(0, 1, 10000), theoretical_quantiles * 100)\n",
    "    \n",
    "    axes[0,1].scatter(normal_quantiles, sorted_returns, alpha=0.6, s=1)\n",
    "    axes[0,1].plot([normal_quantiles.min(), normal_quantiles.max()], \n",
    "                   [sorted_returns.min(), sorted_returns.max()], 'r--', alpha=0.8)\n",
    "    axes[0,1].set_title('Q-Q Plot (Normal Distribution)')\n",
    "    axes[0,1].set_xlabel('Theoretical Quantiles')\n",
    "    axes[0,1].set_ylabel('Sample Quantiles')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Time series (sample every 50th point for performance)\n",
    "    sample_indices = range(0, len(returns), 50)\n",
    "    sample_returns = returns.iloc[sample_indices]\n",
    "    axes[1,0].plot(sample_indices, sample_returns, alpha=0.7, linewidth=0.8, color='blue')\n",
    "    axes[1,0].set_title('Returns Over Time (Sampled)')\n",
    "    axes[1,0].set_xlabel('Time Index')\n",
    "    axes[1,0].set_ylabel('Returns')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rolling volatility (252-day window)\n",
    "    rolling_vol = returns.rolling(window=252).std() * np.sqrt(252)\n",
    "    sample_vol = rolling_vol.iloc[sample_indices]\n",
    "    axes[1,1].plot(sample_indices, sample_vol, color='orange', alpha=0.8, linewidth=1)\n",
    "    axes[1,1].set_title('Rolling Volatility (252-day)')\n",
    "    axes[1,1].set_xlabel('Time Index')\n",
    "    axes[1,1].set_ylabel('Annualized Volatility')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_returns_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce29927-4ad8-4165-bb7b-09d5ab681aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. FEATURE CORRELATION ANALYSIS (Updated)\n",
    "# =========================================\n",
    "def analyze_feature_correlations():\n",
    "    \"\"\"Feature correlation analysis with actual variable names\"\"\"\n",
    "    \n",
    "    # Sample features from each category for correlation analysis\n",
    "    feature_sample = []\n",
    "    categories = ['M', 'E', 'I', 'P', 'V', 'S', 'D']\n",
    "    \n",
    "    print(\"🔍 SELECTED FEATURES FOR CORRELATION ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for cat in categories:\n",
    "        cat_features = [col for col in train_df.columns if col.startswith(cat)]\n",
    "        if cat_features:\n",
    "            # Take up to 3 features per category\n",
    "            selected_features = cat_features[:3]\n",
    "            feature_sample.extend(selected_features)\n",
    "            \n",
    "            print(f\"\\n{cat} Category Features:\")\n",
    "            for i, feature in enumerate(selected_features, 1):\n",
    "                print(f\"  {feature}\")\n",
    "    \n",
    "    # Add target variable\n",
    "    feature_sample.append('forward_returns')\n",
    "    print(f\"\\nTarget Variable:\")\n",
    "    print(f\"  forward_returns\")\n",
    "    \n",
    "    print(f\"\\nTotal features in correlation matrix: {len(feature_sample)}\")\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_data = train_df[feature_sample].corr()\n",
    "    \n",
    "    # Simple correlation heatmap using matplotlib\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    plt.imshow(corr_data, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Correlation')\n",
    "    plt.title('🔗 Feature Correlation Matrix (Sample Features)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add feature names\n",
    "    plt.xticks(range(len(feature_sample)), feature_sample, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(feature_sample)), feature_sample)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show top correlations with target\n",
    "    target_corrs = corr_data['forward_returns'].abs().sort_values(ascending=False)[1:11]  # Top 10\n",
    "    print(\"\\n🎯 TOP 10 FEATURES CORRELATED WITH FORWARD RETURNS\")\n",
    "    print(\"=\" * 60)\n",
    "    for feature, corr in target_corrs.items():\n",
    "        print(f\"{feature:15s}: {corr:+.4f}\")\n",
    "    \n",
    "    # Show what these features actually are\n",
    "    print(f\"\\n📊 FEATURE MEANINGS:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"M = Market Dynamics/Technical features\")\n",
    "    print(\"E = Macro Economic features\") \n",
    "    print(\"I = Interest Rate features\")\n",
    "    print(\"P = Price/Valuation features\")\n",
    "    print(\"V = Volatility features\")\n",
    "    print(\"S = Sentiment features\")\n",
    "    print(\"D = Dummy/Binary features\")\n",
    "\n",
    "analyze_feature_correlations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e313d8-4df0-4612-b800-a88caaed42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. MISSING DATA VISUALIZATION\n",
    "# =============================\n",
    "def plot_missing_data_simple():\n",
    "    \"\"\"Simple missing data analysis\"\"\"\n",
    "    \n",
    "    # Sample data for visualization (every 200th row)\n",
    "    sample_indices = range(0, len(train_df), 200)\n",
    "    sample_df = train_df.iloc[sample_indices]\n",
    "    \n",
    "    # Select representative features from each category\n",
    "    feature_sample = []\n",
    "    categories = ['M', 'E', 'I', 'P', 'V', 'S', 'D']\n",
    "    \n",
    "    for cat in categories:\n",
    "        cat_features = [col for col in train_df.columns if col.startswith(cat)]\n",
    "        if cat_features:\n",
    "            feature_sample.extend(cat_features[:2])  # 2 features per category\n",
    "    \n",
    "    print(f\"📊 Analyzing missing data patterns for {len(feature_sample)} features\")\n",
    "    print(f\"Sampling every 200th row ({len(sample_df)} total samples)\")\n",
    "    \n",
    "    # Calculate missing data percentage over time\n",
    "    missing_pct_over_time = []\n",
    "    window_size = 500  # Calculate missing % for every 500 rows\n",
    "    \n",
    "    for i in range(0, len(train_df) - window_size, window_size):\n",
    "        window_data = train_df.iloc[i:i+window_size]\n",
    "        missing_pct = (window_data[feature_sample].isnull().sum().sum() / \n",
    "                      (len(feature_sample) * len(window_data))) * 100\n",
    "        missing_pct_over_time.append(missing_pct)\n",
    "    \n",
    "    # Plot missing data trend\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x_axis = range(0, len(train_df) - window_size, window_size)\n",
    "    plt.plot(x_axis, missing_pct_over_time, color='red', linewidth=2, marker='o', markersize=4)\n",
    "    plt.title('🔍 Missing Data Percentage Over Time', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Date ID (Trading Day Index)')\n",
    "    plt.ylabel('Missing Data (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for key periods\n",
    "    plt.axhline(y=50, color='orange', linestyle='--', alpha=0.7, label='50% Missing')\n",
    "    plt.axhline(y=10, color='green', linestyle='--', alpha=0.7, label='10% Missing')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📈 MISSING DATA TREND SUMMARY:\")\n",
    "    print(f\"Early period (date_id 0-500): {missing_pct_over_time[0]:.1f}% missing\")\n",
    "    print(f\"Recent period (date_id 8000+): {missing_pct_over_time[-1]:.1f}% missing\")\n",
    "    print(f\"Data quality improvement: {missing_pct_over_time[0] - missing_pct_over_time[-1]:.1f} percentage points\")\n",
    "    print(f\"Total span: ~{8990/252:.1f} years of trading data (assuming 252 trading days/year)\")\n",
    "\n",
    "plot_missing_data_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3063cd-a22a-413e-98fc-89a987c4b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. MARKET REGIME ANALYSIS\n",
    "# =========================\n",
    "def plot_market_regimes():\n",
    "    \"\"\"Analyze different market regimes and volatility clustering\"\"\"\n",
    "    \n",
    "    returns = train_df['forward_returns'].dropna()\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    window = 252  # 1 year window (assuming 252 trading days per year)\n",
    "    rolling_mean = returns.rolling(window=window).mean()\n",
    "    rolling_std = returns.rolling(window=window).std()\n",
    "    rolling_sharpe = rolling_mean / rolling_std * np.sqrt(252)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    fig.suptitle('📊 Market Regime Analysis (252-Day Rolling Windows)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Sample for visualization (every 20th point to reduce clutter)\n",
    "    sample_indices = range(window, len(returns), 20)\n",
    "    \n",
    "    # Rolling returns\n",
    "    axes[0].plot(sample_indices, rolling_mean.iloc[sample_indices] * 252 * 100, \n",
    "                 color='blue', alpha=0.8, linewidth=1)\n",
    "    axes[0].axhline(0, color='red', linestyle='--', alpha=0.7, label='Break-even (0% return)')\n",
    "    axes[0].set_title('Rolling Annualized Returns (252-day window)')\n",
    "    axes[0].set_ylabel('Annualized Return (%)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Rolling volatility\n",
    "    axes[1].plot(sample_indices, rolling_std.iloc[sample_indices] * np.sqrt(252) * 100, \n",
    "                 color='orange', alpha=0.8, linewidth=1)\n",
    "    axes[1].axhline(20, color='green', linestyle='--', alpha=0.7, label='Low volatility (20%)')\n",
    "    axes[1].axhline(30, color='orange', linestyle='--', alpha=0.7, label='High volatility (30%)')\n",
    "    axes[1].set_title('Rolling Annualized Volatility (252-day window)')\n",
    "    axes[1].set_ylabel('Annualized Volatility (%)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Rolling Sharpe ratio\n",
    "    axes[2].plot(sample_indices, rolling_sharpe.iloc[sample_indices], \n",
    "                 color='green', alpha=0.8, linewidth=1)\n",
    "    axes[2].axhline(0, color='red', linestyle='--', alpha=0.7, label='No risk premium (Sharpe = 0)')\n",
    "    axes[2].axhline(1, color='orange', linestyle='--', alpha=0.7, label='Good risk-adjusted return (Sharpe = 1)')\n",
    "    axes[2].set_title('Rolling Sharpe Ratio (252-day window)')\n",
    "    axes[2].set_xlabel('Date ID (Trading Day Index)')\n",
    "    axes[2].set_ylabel('Sharpe Ratio')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Explain what Sharpe Ratio means\n",
    "    print(\"\\n🎯 WHAT IS THE SHARPE RATIO?\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"The Sharpe Ratio measures risk-adjusted returns:\")\n",
    "    print(\"• Formula: (Return - Risk-free rate) / Volatility\")\n",
    "    print(\"• Higher = Better (more return per unit of risk)\")\n",
    "    print(\"• Sharpe > 1.0 = Good risk-adjusted performance\")\n",
    "    print(\"• Sharpe > 2.0 = Excellent risk-adjusted performance\") \n",
    "    print(\"• Sharpe < 0 = Losing money or not beating risk-free rate\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"• If Sharpe = 1.5, you get 1.5% extra return for every 1% of risk\")\n",
    "    print(\"• If Sharpe = 0.5, you only get 0.5% extra return for every 1% of risk\")\n",
    "    \n",
    "    # Print some regime statistics\n",
    "    print(\"\\n📊 MARKET REGIME STATISTICS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calculate regime periods\n",
    "    high_vol_periods = (rolling_std * np.sqrt(252) > 0.25).sum()  # >25% annual vol\n",
    "    negative_return_periods = (rolling_mean < 0).sum()\n",
    "    high_sharpe_periods = (rolling_sharpe > 1.0).sum()\n",
    "    \n",
    "    total_periods = len(rolling_mean.dropna())\n",
    "    \n",
    "    print(f\"High Volatility periods (>25% annual): {high_vol_periods}/{total_periods} ({high_vol_periods/total_periods*100:.1f}%)\")\n",
    "    print(f\"Negative return periods: {negative_return_periods}/{total_periods} ({negative_return_periods/total_periods*100:.1f}%)\")\n",
    "    print(f\"High Sharpe periods (>1.0): {high_sharpe_periods}/{total_periods} ({high_sharpe_periods/total_periods*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nVolatility Range: {(rolling_std * np.sqrt(252)).min()*100:.1f}% - {(rolling_std * np.sqrt(252)).max()*100:.1f}%\")\n",
    "    print(f\"Returns Range: {(rolling_mean * 252).min()*100:.1f}% - {(rolling_mean * 252).max()*100:.1f}%\")\n",
    "\n",
    "plot_market_regimes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b6097-bf16-4dbb-8d12-afd11d5b4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. EXTREME EVENTS ANALYSIS\n",
    "# ===========================\n",
    "def analyze_extreme_events():\n",
    "    \"\"\"Identify and analyze extreme market events\"\"\"\n",
    "    \n",
    "    returns = train_df['forward_returns'].dropna()\n",
    "    \n",
    "    # Define extreme events (>2.5 standard deviations)\n",
    "    threshold = 2.5 * returns.std()\n",
    "    extreme_positive = returns[returns > threshold]\n",
    "    extreme_negative = returns[returns < -threshold]\n",
    "    normal_returns = returns[(returns <= threshold) & (returns >= -threshold)]\n",
    "    \n",
    "    print(\"\\n⚡ EXTREME MARKET EVENTS ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Extreme events defined as >2.5 standard deviations from mean\")\n",
    "    print(f\"Threshold: ±{threshold*100:.3f}% daily return\")\n",
    "    \n",
    "    # Create extreme events summary table\n",
    "    print(f\"\\n{'Event Type':<20} {'Count':<8} {'Percentage':<12} {'Avg Return':<12} {'Max/Min Return':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'Extreme Positive':<20} {len(extreme_positive):<8} {len(extreme_positive)/len(returns)*100:>10.2f}% {extreme_positive.mean()*100:>10.3f}% {extreme_positive.max()*100:>13.3f}%\")\n",
    "    print(f\"{'Extreme Negative':<20} {len(extreme_negative):<8} {len(extreme_negative)/len(returns)*100:>10.2f}% {extreme_negative.mean()*100:>10.3f}% {extreme_negative.min()*100:>13.3f}%\")\n",
    "    print(f\"{'Normal Days':<20} {len(normal_returns):<8} {len(normal_returns)/len(returns)*100:>10.2f}% {normal_returns.mean()*100:>10.3f}% {'N/A':>13}\")\n",
    "    \n",
    "    # Visualize extreme events distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('📊 Extreme Market Events Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Histogram showing extreme events\n",
    "    axes[0].hist(returns, bins=100, alpha=0.7, color='lightblue', edgecolor='black', label='All Returns')\n",
    "    axes[0].hist(extreme_positive, bins=20, alpha=0.8, color='green', label=f'Extreme Positive ({len(extreme_positive)} days)')\n",
    "    axes[0].hist(extreme_negative, bins=20, alpha=0.8, color='red', label=f'Extreme Negative ({len(extreme_negative)} days)')\n",
    "    axes[0].axvline(threshold, color='orange', linestyle='--', label=f'±{threshold*100:.2f}% threshold')\n",
    "    axes[0].axvline(-threshold, color='orange', linestyle='--')\n",
    "    axes[0].set_title('Returns Distribution with Extreme Events')\n",
    "    axes[0].set_xlabel('Daily Returns')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Time series showing when extreme events occurred\n",
    "    extreme_events_series = pd.Series(0, index=returns.index)\n",
    "    extreme_events_series[extreme_positive.index] = 1  # Positive extremes\n",
    "    extreme_events_series[extreme_negative.index] = -1  # Negative extremes\n",
    "    \n",
    "    # Sample every 50th point for visualization\n",
    "    sample_indices = range(0, len(extreme_events_series), 50)\n",
    "    sample_events = extreme_events_series.iloc[sample_indices]\n",
    "    sample_returns = returns.iloc[sample_indices]\n",
    "    \n",
    "    axes[1].scatter(sample_indices, sample_returns, c=sample_events, cmap='RdYlGn', \n",
    "                   alpha=0.6, s=20, edgecolors='black', linewidth=0.5)\n",
    "    axes[1].axhline(threshold, color='orange', linestyle='--', alpha=0.7, label=f'±{threshold*100:.2f}% threshold')\n",
    "    axes[1].axhline(-threshold, color='orange', linestyle='--', alpha=0.7)\n",
    "    axes[1].axhline(0, color='gray', linestyle='-', alpha=0.5)\n",
    "    axes[1].set_title('Extreme Events Over Time')\n",
    "    axes[1].set_xlabel('Date ID (Trading Day Index)')\n",
    "    axes[1].set_ylabel('Daily Returns')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Top 10 best and worst days\n",
    "    print(\"\\n📈 TOP 10 BEST TRADING DAYS\")\n",
    "    print(\"-\" * 40)\n",
    "    best_days = returns.nlargest(10)\n",
    "    for i, (date_idx, ret) in enumerate(best_days.items(), 1):\n",
    "        print(f\"{i:2d}. Date ID {date_idx:4d}: {ret*100:+.3f}%\")\n",
    "    \n",
    "    print(\"\\n📉 TOP 10 WORST TRADING DAYS\")\n",
    "    print(\"-\" * 40)\n",
    "    worst_days = returns.nsmallest(10)\n",
    "    for i, (date_idx, ret) in enumerate(worst_days.items(), 1):\n",
    "        print(f\"{i:2d}. Date ID {date_idx:4d}: {ret*100:+.3f}%\")\n",
    "    \n",
    "    # Additional insights\n",
    "    print(\"\\n🔍 EXTREME EVENTS INSIGHTS:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"• Extreme events occur roughly {(len(extreme_positive) + len(extreme_negative))/len(returns)*100:.2f}% of the time\")\n",
    "    print(f\"• Negative extremes are {abs(extreme_negative.mean())/extreme_positive.mean():.1f}x larger on average than positive extremes\")\n",
    "    print(f\"• Worst single day: {extreme_negative.min()*100:.2f}%\")\n",
    "    print(f\"• Best single day: {extreme_positive.max()*100:.2f}%\")\n",
    "    \n",
    "    if len(extreme_negative) > len(extreme_positive):\n",
    "        print(f\"• More extreme negative days ({len(extreme_negative)}) than positive ({len(extreme_positive)}) - typical of equity markets\")\n",
    "    else:\n",
    "        print(f\"• More extreme positive days ({len(extreme_positive)}) than negative ({len(extreme_negative)}) - unusual pattern\")\n",
    "\n",
    "analyze_extreme_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b98434-b4c5-4305-94e6-d96db27a1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. ANALYSIS SUMMARY AND RECOMMENDATIONS\n",
    "# ========================================\n",
    "def print_analysis_summary():\n",
    "    \"\"\"Print comprehensive analysis summary and modeling recommendations\"\"\"\n",
    "    \n",
    "    # Calculate some key metrics for the summary\n",
    "    returns = train_df['forward_returns'].dropna()\n",
    "    total_features = train_df.shape[1] - 4  # Exclude date_id + 3 targets\n",
    "    \n",
    "    # Missing data summary\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']]\n",
    "    overall_missing = (train_df[feature_cols].isnull().sum().sum() / (len(feature_cols) * len(train_df))) * 100\n",
    "    \n",
    "    # Early vs recent period missing data\n",
    "    period_size = len(train_df) // 4\n",
    "    early_missing = (train_df.iloc[:period_size][feature_cols].isnull().sum().sum() / (len(feature_cols) * period_size)) * 100\n",
    "    recent_missing = (train_df.iloc[-period_size:][feature_cols].isnull().sum().sum() / (len(feature_cols) * period_size)) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🎯 COMPREHENSIVE ANALYSIS SUMMARY & MODELING RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n📊 DATASET CHARACTERISTICS:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"• {len(train_df):,} trading days spanning ~{len(train_df)/252:.1f} years of market history\")\n",
    "    print(f\"• {total_features} engineered features across 7 categories (M,E,I,P,V,S,D)\")\n",
    "    print(f\"• Target: Next-day S&P 500 returns with {returns.mean()*100:.3f}% average daily return\")\n",
    "    print(f\"• Daily volatility: {returns.std()*100:.2f}% (annualized: {returns.std()*np.sqrt(252)*100:.1f}%)\")\n",
    "    print(f\"• Overall missing data: {overall_missing:.1f}%\")\n",
    "    print(f\"• Missing data evolution: {early_missing:.1f}% (early) → {recent_missing:.1f}% (recent)\")\n",
    "    \n",
    "    print(\"\\n📈 KEY MARKET PATTERNS DISCOVERED:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Calculate some statistics\n",
    "    extreme_threshold = 2.5 * returns.std()\n",
    "    extreme_days = len(returns[(returns > extreme_threshold) | (returns < -extreme_threshold)])\n",
    "    positive_days = len(returns[returns > 0])\n",
    "    \n",
    "    print(f\"• {positive_days/len(returns)*100:.1f}% of days are positive (typical for equity markets)\")\n",
    "    print(f\"• {extreme_days} extreme events (>{extreme_threshold*100:.2f}% daily moves)\")\n",
    "    print(f\"• Largest single-day gain: {returns.max()*100:.2f}%\")\n",
    "    print(f\"• Largest single-day loss: {returns.min()*100:.2f}%\")\n",
    "    print(f\"• Returns show {'positive' if returns.mean() > 0 else 'negative'} skew (typical for stocks)\")\n",
    "    \n",
    "    print(\"\\n🧠 MODELING STRATEGY RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n1. 📋 DATA PREPROCESSING PRIORITIES:\")\n",
    "    print(\"   • Implement time-aware missing data imputation (forward-fill, interpolation)\")\n",
    "    print(\"   • Consider creating 'data availability' features (missing data patterns as signals)\")\n",
    "    print(\"   • Split dataset into time periods based on data quality for ensemble modeling\")\n",
    "    print(\"   • Handle outliers carefully - extreme events contain valuable information\")\n",
    "    \n",
    "    print(\"\\n2. 🔧 FEATURE ENGINEERING STRATEGIES:\")\n",
    "    print(\"   • Create lagged features (1, 2, 5, 10 day lags) for all categories\")\n",
    "    print(\"   • Build rolling statistics (mean, std, min, max) with multiple windows\")\n",
    "    print(\"   • Combine features within categories (e.g., sentiment composite score)\")\n",
    "    print(\"   • Add volatility regime indicators and market cycle features\")\n",
    "    print(\"   • Create interaction features between different categories\")\n",
    "    \n",
    "    print(\"\\n3. 🤖 MODEL SELECTION APPROACH:\")\n",
    "    print(\"   • Primary: Ensemble methods (Random Forest, XGBoost, LightGBM)\")\n",
    "    print(\"   • Secondary: Deep learning (LSTM, GRU for time series patterns)\")\n",
    "    print(\"   • Advanced: Transformer models for long-range dependencies\")\n",
    "    print(\"   • Consider regime-switching models for different market conditions\")\n",
    "    print(\"   • Meta-learning: Combine multiple model predictions\")\n",
    "    \n",
    "    print(\"\\n4. ✅ VALIDATION STRATEGY (CRITICAL):\")\n",
    "    print(\"   • Use time series cross-validation (NEVER shuffle data)\")\n",
    "    print(\"   • Implement walk-forward validation with expanding window\")\n",
    "    print(\"   • Test on multiple market regimes (bull, bear, crisis periods)\")\n",
    "    print(\"   • Validate on out-of-sample recent data\")\n",
    "    print(\"   • Use multiple metrics: Sharpe ratio, directional accuracy, max drawdown\")\n",
    "    \n",
    "    print(\"\\n5. ⚠️  KEY CHALLENGES TO EXPECT:\")\n",
    "    print(\"   • Low signal-to-noise ratio (correlations ~0.04 are typical)\")\n",
    "    print(\"   • Non-stationarity: market patterns change over time\")\n",
    "    print(\"   • Missing data bias: early period models may not generalize\")\n",
    "    print(\"   • Extreme events: fat tails and black swan events\")\n",
    "    print(\"   • Regime changes: models may fail during market transitions\")\n",
    "    \n",
    "    print(\"\\n6. 🎯 SUCCESS METRICS & EXPECTATIONS:\")\n",
    "    print(\"   • Target: Sharpe ratio > 1.0 (good risk-adjusted returns)\")\n",
    "    print(\"   • Directional accuracy > 55% (better than coin flip)\")\n",
    "    print(\"   • Maximum drawdown < 10% (risk management)\")\n",
    "    print(\"   • Consistent performance across different market regimes\")\n",
    "    print(\"   • Remember: Even small improvements are valuable in finance!\")\n",
    "    \n",
    "    print(\"\\n7. 📊 NEXT STEPS ROADMAP:\")\n",
    "    print(\"   • Start with simple baseline models (linear regression, random forest)\")\n",
    "    print(\"   • Implement robust cross-validation framework\")\n",
    "    print(\"   • Build feature engineering pipeline\")\n",
    "    print(\"   • Test ensemble approaches\")\n",
    "    print(\"   • Add sophisticated models incrementally\")\n",
    "    print(\"   • Focus on risk management and position sizing\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"💡 REMEMBER: In financial markets, predicting direction is often\")\n",
    "    print(\"   more valuable than predicting exact magnitude!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print_analysis_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f071648-60f5-4b24-b35a-7ab598eda539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
